{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "import unicodedata\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification():\n",
    "    \n",
    "     # ----------------------------------------- Constructor -----------------------------------------\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.punctuation = set(punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.SMILEY)\n",
    "        self.stopword_list = set(stopwords.words('english'))\n",
    "        unwanted_stopwords = {'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'couldn', 'what', 'which', 'who',\n",
    "                              'whom', 'why', 'how', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\n",
    "                              \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn',\n",
    "                              \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn',\n",
    "                              \"shouldn't\", 'wasn',\"wasn't\",'weren', \"weren't\", 'won', \"won't\", 'wouldn',\n",
    "                              \"wouldn't\", 'don', \"don't\"}\n",
    "\n",
    "        self.stopword_list = [x for x in self.stopword_list if x not in unwanted_stopwords]\n",
    "        \n",
    "        \n",
    "    # ---------------------------------------- Read Data ----------------------------------------\n",
    "    \n",
    "    def read_data(self, path):\n",
    "        df = pd.read_csv(path, usecols=['tweet', 'label'])\n",
    "        df = df[pd.notnull(df.tweet)]\n",
    "        df = df.sample(frac=1)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "     # ----------------------------------------- Clean Data -----------------------------------------\n",
    "    \n",
    "    def clean_data(self, tweets):\n",
    "        cleaned_tweets = []\n",
    "        for text in tweets:\n",
    "            \n",
    "            # Clean tweet\n",
    "            text = p.clean(text)\n",
    "            \n",
    "            # Remove special characters\n",
    "            text = re.sub(r'(\\\\x(.)*)', '',text)\n",
    "            text = re.sub(r'\\\\n|\\\\t|\\\\n\\\\n', ' ', text)\n",
    "            text = re.sub(r\"b'RT|b'|b RT|b\\\"RT\", \"\", text)\n",
    "            text = re.sub(\"[@#$%^&*)(}{|/><=+=_:\\\"\\\\\\\\]+\",\" \",text).strip()\n",
    "            \n",
    "            #Remove punctuation marks\n",
    "            text = \"\".join(x for x in text if x not in self.punctuation)\n",
    "            \n",
    "            # Remove accented words\n",
    "            text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            \n",
    "            # Splitting Hashtag words\n",
    "            text = \" \".join([x for x in re.split('([A-Z][a-z]+)', text) if x])\n",
    "            \n",
    "            # Remove long spaces\n",
    "            pattern = r'^\\s*|\\s\\s*'\n",
    "            text = re.sub(pattern, ' ', text).strip()\n",
    "            \n",
    "            # Remove numbers\n",
    "            text = re.sub('[0-9]+', '', text)\n",
    "            \n",
    "            cleaned_tweets.append(text)\n",
    "        \n",
    "        return cleaned_tweets\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------- Preprocess Data -----------------------------------------\n",
    "    \n",
    "    def preprocess_data(self, tweets):\n",
    "        preprocessed_tweets = []\n",
    "        for text in tweets:\n",
    "            \n",
    "            # Remove stopwords\n",
    "            text = \" \".join(x for x in text.lower().split() if x not in self.stopword_list)\n",
    "            \n",
    "            # Text Lemmatization\n",
    "            lemmatized_words = []\n",
    "            for word in text.split():\n",
    "                word1 = self.lemmatizer.lemmatize(word, pos=\"n\")\n",
    "                word2 = self.lemmatizer.lemmatize(word1, pos=\"v\")\n",
    "                word3 = self.lemmatizer.lemmatize(word2, pos=(\"a\"))\n",
    "                lemmatized_words.append(word3)\n",
    "            text = \" \".join(x for x in lemmatized_words)\n",
    "            \n",
    "            preprocessed_tweets.append(text)\n",
    "            \n",
    "        return preprocessed_tweets\n",
    "    \n",
    "    \n",
    "    # ------------------------------ Word-level unigram TF-IDF Vectorization ------------------------------\n",
    "    \n",
    "    def tfidf_vectorize(self, X_train, x_test):\n",
    "        tfidf_vec = TfidfVectorizer(sublinear_tf=True, min_df=3, norm='l2', stop_words='english')\n",
    "        tfidf_vec.fit(X_train)\n",
    "        X_train_tfidf = tfidf_vec.transform(X_train).toarray()  \n",
    "        X_test_tfidf = tfidf_vec.transform(X_test).toarray()      \n",
    "        return tfidf_vec, X_train_tfidf, X_test_tfidf\n",
    "    \n",
    "    \n",
    "    # ------------------------------------ Train Model ------------------------------------\n",
    "    \n",
    "    def train_model(self, classifier, X_train, X_test, y_train, y_test):\n",
    "        model = classifier.fit(X_train, y_train)\n",
    "        results = model.predict(X_test)\n",
    "        return model, metrics.accuracy_score(results, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = Classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8556</th>\n",
       "      <td>never look anybody unless youre help life quote</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>great terry fox pathetic conclude no support k...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4590</th>\n",
       "      <td>algeria</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4791</th>\n",
       "      <td>great terry fox pathetic annoy khalistan refer...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8972</th>\n",
       "      <td>marvelous positive affirmation</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8454</th>\n",
       "      <td>hey love music really not youre perform murder...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6281</th>\n",
       "      <td>anyone else morning meet amp veep</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>b khalistan answer india anti farmer bill sfj ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6852</th>\n",
       "      <td>ever felt like nba fix bamboosled market nbafi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>b account withhold india response legal demand...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10529 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  label\n",
       "8556    never look anybody unless youre help life quote    0.0\n",
       "4004  great terry fox pathetic conclude no support k...    1.0\n",
       "4590                                            algeria    1.0\n",
       "4791  great terry fox pathetic annoy khalistan refer...    1.0\n",
       "8972                     marvelous positive affirmation    0.0\n",
       "...                                                 ...    ...\n",
       "8454  hey love music really not youre perform murder...    0.0\n",
       "6281                  anyone else morning meet amp veep    0.0\n",
       "4521  b khalistan answer india anti farmer bill sfj ...    1.0\n",
       "6852  ever felt like nba fix bamboosled market nbafi...    0.0\n",
       "1561  b account withhold india response legal demand...    1.0\n",
       "\n",
       "[10529 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"dataset/combined_dataset/data_main.csv\"\n",
    "data = cl.read_data(path)\n",
    "data = data.sample(frac=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.tweet.values.tolist(), \n",
    "                                                    data.label,\n",
    "                                                    test_size=0.3,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7370\n",
      "7370\n",
      "3159\n",
      "3159\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec, X_train_tfidf, X_test_tfidf = cl.tfidf_vectorize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7370, 2167)\n",
      "(3159, 2167)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Count Vectors:  98.32225387780943\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "NB_model, NB_accuracy = cl.train_model(MultinomialNB(), X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "print(\"Naive Bayes, Count Vectors: \", NB_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(NB_model, \"models/NB_tfidf.pk1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Count Vectors:  99.08198797087687\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "LinearSVC_model, LinearSVC_accuracy = cl.train_model(LinearSVC(), X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "print(\"Naive Bayes, Count Vectors: \", LinearSVC_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(LinearSVC_model, \"models/LinearSVC_tfidf.pk1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\"#Punjab should be given its #Freedom as soon as possible. #Khalistan #Referendum2020 #FreePunjab\",\n",
    "            \"Donald Trump would not win this year's elections. #Trump\",\n",
    "           \"ISRO makes a giant leap forward by sending satellites to mars. #ISRO #MissionMangal\",\n",
    "            \"The new Pime minister fellowship program will benefit many students.\",\n",
    "            \"The #Khalistan movement is gaining momentum.. #India is falling apart.\",\n",
    "           \"The  so-called pure country called Pakistan is killing, murdering, blasting, and commuting inhuman atrocities on their own Muslim brotherhood. Do the Sikhs want the same treatment meted out for themselves? #Khalistan is just an anti-India agenda of Pakistan, stop demanding it\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2167)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_test_set = cl.clean_data(test_set)\n",
    "preprocessed_test_set = cl.preprocess_data(cleaned_test_set)\n",
    "tfidf_test_set = tfidf_vec.transform(preprocessed_test_set).toarray()\n",
    "tfidf_test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khalistan\n",
      "General\n",
      "General\n",
      "General\n",
      "Khalistan\n",
      "Khalistan\n"
     ]
    }
   ],
   "source": [
    "NB_result = NB_model.predict(tfidf_test_set)\n",
    "for r in NB_result:\n",
    "    if r == 1:\n",
    "        print(\"Khalistan\")\n",
    "    if r == 0:\n",
    "        print(\"General\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khalistan\n",
      "General\n",
      "General\n",
      "General\n",
      "Khalistan\n",
      "Khalistan\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_result = LinearSVC_model.predict(tfidf_test_set)\n",
    "for r in LinearSVC_result:\n",
    "    if r == 1:\n",
    "        print(\"Khalistan\")\n",
    "    if r == 0:\n",
    "        print(\"General\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
