{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "import unicodedata\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification():\n",
    "    \n",
    "     # ----------------------------------------- Constructor -----------------------------------------\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.punctuation = set(punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.SMILEY)\n",
    "        self.stopword_list = set(stopwords.words('english'))\n",
    "        unwanted_stopwords = {'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'couldn', 'what', 'which', 'who',\n",
    "                              'whom', 'why', 'how', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\n",
    "                              \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn',\n",
    "                              \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn',\n",
    "                              \"shouldn't\", 'wasn',\"wasn't\",'weren', \"weren't\", 'won', \"won't\", 'wouldn',\n",
    "                              \"wouldn't\", 'don', \"don't\"}\n",
    "\n",
    "        self.stopword_list = [x for x in self.stopword_list if x not in unwanted_stopwords]\n",
    "        \n",
    "        \n",
    "    # ---------------------------------------- Read Data ----------------------------------------\n",
    "    \n",
    "    def read_data(self, path):\n",
    "        df = pd.read_csv(path, usecols=['tweet', 'label'])\n",
    "        df = df[pd.notnull(df.tweet)]\n",
    "        df = df.sample(frac=1)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "     # ----------------------------------------- Clean Data -----------------------------------------\n",
    "    \n",
    "    def clean_data(self, tweets):\n",
    "        cleaned_tweets = []\n",
    "        for text in tweets:\n",
    "            \n",
    "            # Clean tweet\n",
    "            text = p.clean(text)\n",
    "            \n",
    "            # Remove special characters\n",
    "            text = re.sub(r'(\\\\x(.)*)', '',text)\n",
    "            text = re.sub(r'\\\\n|\\\\t|\\\\n\\\\n', ' ', text)\n",
    "            text = re.sub(r\"b'RT|b'|b RT|b\\\"RT\", \"\", text)\n",
    "            text = re.sub(\"[@#$%^&*)(}{|/><=+=_:\\\"\\\\\\\\]+\",\" \",text).strip()\n",
    "            \n",
    "            #Remove punctuation marks\n",
    "            text = \"\".join(x for x in text if x not in self.punctuation)\n",
    "            \n",
    "            # Remove accented words\n",
    "            text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            \n",
    "            # Splitting Hashtag words\n",
    "            text = \" \".join([x for x in re.split('([A-Z][a-z]+)', text) if x])\n",
    "            \n",
    "            # Remove long spaces\n",
    "            pattern = r'^\\s*|\\s\\s*'\n",
    "            text = re.sub(pattern, ' ', text).strip()\n",
    "            \n",
    "            # Remove numbers\n",
    "            text = re.sub('[0-9]+', '', text)\n",
    "            \n",
    "            cleaned_tweets.append(text)\n",
    "        \n",
    "        return cleaned_tweets\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------- Preprocess Data -----------------------------------------\n",
    "    \n",
    "    def preprocess_data(self, tweets):\n",
    "        preprocessed_tweets = []\n",
    "        for text in tweets:\n",
    "            \n",
    "            # Remove stopwords\n",
    "            text = \" \".join(x for x in text.lower().split() if x not in self.stopword_list)\n",
    "            \n",
    "            # Text Lemmatization\n",
    "            lemmatized_words = []\n",
    "            for word in text.split():\n",
    "                word1 = self.lemmatizer.lemmatize(word, pos=\"n\")\n",
    "                word2 = self.lemmatizer.lemmatize(word1, pos=\"v\")\n",
    "                word3 = self.lemmatizer.lemmatize(word2, pos=(\"a\"))\n",
    "                lemmatized_words.append(word3)\n",
    "            text = \" \".join(x for x in lemmatized_words)\n",
    "            \n",
    "            preprocessed_tweets.append(text)\n",
    "            \n",
    "        return preprocessed_tweets\n",
    "    \n",
    "    \n",
    "    # ------------------------------ Word-level unigram TF-IDF Vectorization ------------------------------\n",
    "    \n",
    "    def tfidf_vectorize(self, X_train, x_test):\n",
    "        tfidf_vec = TfidfVectorizer(sublinear_tf=True, min_df=3, norm='l2', stop_words='english')\n",
    "        tfidf_vec.fit(X_train)\n",
    "        X_train_tfidf = tfidf_vec.transform(X_train).toarray()  \n",
    "        X_test_tfidf = tfidf_vec.transform(X_test).toarray()      \n",
    "        return tfidf_vec, X_train_tfidf, X_test_tfidf\n",
    "    \n",
    "    \n",
    "    # ------------------------------------ Train Model ------------------------------------\n",
    "    \n",
    "    def train_model(self, classifier, X_train, X_test, y_train, y_test):\n",
    "        model = classifier.fit(X_train, y_train)\n",
    "        results = model.predict(X_test)\n",
    "        return model, metrics.accuracy_score(results, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = Classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9147</th>\n",
       "      <td>goodnight sweetdreams wish youall space happy ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4815</th>\n",
       "      <td>great terry fox pathetic annoy khalistan refer...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>voice people gilgit baltistan outright autonom...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>sfj khalistan gharaomodi bandhbharat mot men a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6985</th>\n",
       "      <td>find via how train dragon serpent heir due feb</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10464</th>\n",
       "      <td>yet play doctor faustus kit harington ilsleepw...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4515</th>\n",
       "      <td>b khalistan answer india anti farmer bill sfj ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>farmer every year commit suicide indian govern...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10455</th>\n",
       "      <td>today day wale v england togetherstronger cmon...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>sfj conduct referendum different place india a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10529 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  label\n",
       "9147   goodnight sweetdreams wish youall space happy ...    0.0\n",
       "4815   great terry fox pathetic annoy khalistan refer...    1.0\n",
       "2132   voice people gilgit baltistan outright autonom...    1.0\n",
       "1241   sfj khalistan gharaomodi bandhbharat mot men a...    1.0\n",
       "6985      find via how train dragon serpent heir due feb    0.0\n",
       "...                                                  ...    ...\n",
       "10464  yet play doctor faustus kit harington ilsleepw...    0.0\n",
       "4515   b khalistan answer india anti farmer bill sfj ...    1.0\n",
       "1159   farmer every year commit suicide indian govern...    1.0\n",
       "10455  today day wale v england togetherstronger cmon...    0.0\n",
       "2207   sfj conduct referendum different place india a...    1.0\n",
       "\n",
       "[10529 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"dataset/combined_dataset/data_main.csv\"\n",
    "data = cl.read_data(path)\n",
    "data = data.sample(frac=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.tweet.values.tolist(), \n",
    "                                                    data.label,\n",
    "                                                    test_size=0.3,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7370\n",
      "7370\n",
      "3159\n",
      "3159\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec, X_train_tfidf, X_test_tfidf = cl.tfidf_vectorize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7370, 2129)\n",
      "(3159, 2129)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy:  98.06900918012029\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "NB_model, NB_accuracy = cl.train_model(MultinomialNB(), X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "print(\"Naive Bayes Accuracy: \", NB_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/NB_tfidf.pk1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(NB_model, \"models/NB_tfidf.pk1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Accuracy:  99.1769547325103\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC on Word Level TF IDF Vectors\n",
    "LinearSVC_model, LinearSVC_accuracy = cl.train_model(LinearSVC(), X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "print(\"Linear SVC Accuracy: \", LinearSVC_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/LinearSVC_tfidf.pk1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(LinearSVC_model, \"models/LinearSVC_tfidf.pk1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:  98.29059829059828\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression on Word Level TF IDF Vectors\n",
    "LogisticRegression_model, LogisticRegression_accuracy = cl.train_model(LogisticRegression(), X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "print(\"Logistic Regression Accuracy: \", LogisticRegression_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/LogisticRegression_tfidf.pk1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(LogisticRegression_model, \"models/LogisticRegression_tfidf.pk1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy:  99.113643558088\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree on Word Level TF IDF Vectors\n",
    "DecisionTree_model, DecisionTree_accuracy = cl.train_model(DecisionTreeClassifier(), X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "print(\"Decision Tree Accuracy: \", DecisionTree_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/DecisionTree_tfidf.pk1']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(DecisionTree_model, \"models/DecisionTree_tfidf.pk1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy:  99.36688825577714\n"
     ]
    }
   ],
   "source": [
    "# Random Forest on Word Level TF IDF Vectors\n",
    "RandomForest_model, RandomForest_accuracy = cl.train_model(RandomForestClassifier(n_estimators=100), X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "print(\"Decision Tree Accuracy: \", RandomForest_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/RandomForest_tfidf.pk1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(RandomForest_model, \"models/RandomForest_tfidf.pk1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\"#Punjab should be given its #Freedom as soon as possible. #Khalistan #Referendum2020 #FreePunjab\",\n",
    "            \"Donald Trump would not win this year's elections. #Trump\",\n",
    "           \"ISRO makes a giant leap forward by sending satellites to mars. #ISRO #MissionMangal\",\n",
    "            \"The new Pime minister fellowship program will benefit many students.\",\n",
    "            \"The #Khalistan movement is gaining momentum.. #India is falling apart.\",\n",
    "           \"The  so-called pure country called Pakistan is killing, murdering, blasting, and commuting inhuman atrocities on their own Muslim brotherhood. Do the Sikhs want the same treatment meted out for themselves? #Khalistan is just an anti-India agenda of Pakistan, stop demanding it\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2129)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_test_set = cl.clean_data(test_set)\n",
    "preprocessed_test_set = cl.preprocess_data(cleaned_test_set)\n",
    "tfidf_test_set = tfidf_vec.transform(preprocessed_test_set).toarray()\n",
    "tfidf_test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khalistan\n",
      "General\n",
      "General\n",
      "General\n",
      "Khalistan\n",
      "Khalistan\n"
     ]
    }
   ],
   "source": [
    "NB_result = NB_model.predict(tfidf_test_set)\n",
    "for r in NB_result:\n",
    "    if r == 1:\n",
    "        print(\"Khalistan\")\n",
    "    if r == 0:\n",
    "        print(\"General\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khalistan\n",
      "General\n",
      "General\n",
      "General\n",
      "Khalistan\n",
      "Khalistan\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_result = LinearSVC_model.predict(tfidf_test_set)\n",
    "for r in LinearSVC_result:\n",
    "    if r == 1:\n",
    "        print(\"Khalistan\")\n",
    "    if r == 0:\n",
    "        print(\"General\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khalistan\n",
      "General\n",
      "General\n",
      "General\n",
      "Khalistan\n",
      "Khalistan\n"
     ]
    }
   ],
   "source": [
    "LR_result = LogisticRegression_model.predict(tfidf_test_set)\n",
    "for r in LR_result:\n",
    "    if r == 1:\n",
    "        print(\"Khalistan\")\n",
    "    if r == 0:\n",
    "        print(\"General\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khalistan\n",
      "General\n",
      "General\n",
      "General\n",
      "Khalistan\n",
      "Khalistan\n"
     ]
    }
   ],
   "source": [
    "DR_result = DecisionTree_model.predict(tfidf_test_set)\n",
    "for r in DR_result:\n",
    "    if r == 1:\n",
    "        print(\"Khalistan\")\n",
    "    if r == 0:\n",
    "        print(\"General\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khalistan\n",
      "General\n",
      "General\n",
      "General\n",
      "Khalistan\n",
      "Khalistan\n"
     ]
    }
   ],
   "source": [
    "RF_result = RandomForest_model.predict(tfidf_test_set)\n",
    "for r in RF_result:\n",
    "    if r == 1:\n",
    "        print(\"Khalistan\")\n",
    "    if r == 0:\n",
    "        print(\"General\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
